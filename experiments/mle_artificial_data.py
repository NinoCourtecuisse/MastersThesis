import argparse

import torch
from torch import distributions as D
import matplotlib.pyplot as plt
import math

from src.utils.priors import IndependentPrior, CevPrior, NigPrior
from src.utils.distributions import ScaledBeta

"""
Maximum Likelihood Estimation of some models on artificial data generated by the same model.

Implemented models: bs, cev, nig.

Usage:
    ./run.sh experiments/mle_artificial_data.py --model bs
"""

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, choices=['bs', 'cev', 'nig'], required=True)
    parser.add_argument('--seed', type=int, default=0)
    return parser.parse_args()

def main(args):
    # === Instantiate the model ===
    # Need to define a prior to map the parameters to an unconstrained space.
    # This prior is only used to define a support.
    match args.model:
        case 'bs':
            from src.models import Bs as Model
            params_true = torch.tensor([
                [0.1, 0.2]      # mu, sigma to generate artificial data
            ])
            prior = IndependentPrior([
                D.Uniform(-0.5, 0.5),
                D.Uniform(1e-4, 1.0)
            ])
        case 'cev':
            from src.models import Cev as Model
            params_true = torch.tensor([
                [0.1, 2.0, 0.5]     # mu, delta, beta
            ])
            prior = CevPrior(
                mu_dist = D.Uniform(-0.5, 0.5),
                beta_dist = ScaledBeta(5., 5., low=torch.tensor(0.5), high=torch.tensor(2.0)),
                v = 0.2, S=100
            )
        case 'nig':
            from src.models import Nig as Model
            params_true = torch.tensor([
                [0.1, 0.2, -1.0, 0.01]      # mu, sigma, xi, eta
            ])
            prior = NigPrior(
                mu_dist=D.Uniform(-0.5, 0.5),
                sigma_dist=D.LogNormal(math.log(0.2), 1.0),
                theta_eta=-math.log(0.01) / 0.1,
                theta_xi=-math.log(0.001) / 5
            )
        case _:
            return NotImplementedError(f'Model {args.model} is currently not implemented.')

    torch.manual_seed(args.seed)
    dt = 1 / 252
    model = Model(dt, prior)

    # === Generate paths ===
    T = torch.tensor(1.0)
    s0 = torch.tensor(100.0)

    S = model.simulate(params_true, s0, T, M=1000)

    fig1 = plt.figure(figsize=(8, 5))
    plt.plot(torch.linspace(0, T, len(S)), S)
    fig1.tight_layout()

    # === Maximize the log-likelihood for each path ===
    n_paths = S.shape[1]
    n_params = len(params_true.squeeze())
    stats = {
        'mle': torch.zeros(size=(n_paths, n_params)),   # Store the mle obtained based on each path
        'n_it': torch.zeros(size=(n_paths,)),           # Number of iteration before convergence
        'no_convergence': 0,                            # Number of times convergence was not reached before max_it steps.
    }
    grad_norm_threshold = 0.1   # Convergence if grad_norm < threshold.
    lr = 0.5
    max_it = 500
    for i in range(n_paths):
        print(f'Path {i+1} / {n_paths}')
        data = S[:, i]

        if args.model == 'cev': lr = 1.0

        params = prior.sample()     # Sample the initial guess from the prior
        u_params = model.transform.inv(params).requires_grad_(True)     # Map to unconstrained space

        optimizer = torch.optim.Adam([u_params], lr=lr)
        for j in range(max_it):
            optimizer.zero_grad()
            loss = - model.ll(u_params, data)
            loss.backward()

            grad_norm = torch.norm(u_params.grad)

            if grad_norm < grad_norm_threshold:     # Stop the optimization
                stats['n_it'][i] = j
                break
            if j == max_it - 1:
                stats['n_it'][i] = j
                stats['no_convergence'] += 1
                print('Maximum iteration reached.')
            optimizer.step()

        final_params = model.transform.to(u_params)     # Map back to the original space
        stats['mle'][i, :] = final_params[0, :]

    ######## Compute some statistics ########
    mean = torch.mean(stats['mle'], dim=0)
    std = torch.std(stats['mle'], dim=0)
    avg_it = torch.mean(stats['n_it'])
    n_no_conv = stats['no_convergence']

    print(f'Mean: {mean}')
    print(f'Std: {std}')
    print(f'Average number of it: {avg_it}')
    print(f'No convergence: {n_no_conv}')

    plt.show()

if __name__ == '__main__':
    args = parse_args()
    main(args)
